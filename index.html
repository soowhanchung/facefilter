<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>FaceFilter: Audio-visual Speech Separation using still images</title>
	<meta name="author" content="CaffNet">

	<link href="./bootstrap.min.css" rel="stylesheet">
    <link href="./style.css" rel="stylesheet">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
</head>

  <body>

    <div class="container">
      <div class="header">
        <div style="font-size: 10px">
        <h1> <center> FaceFilter: Audio-visual Speech Separation using still images</center> </h1>
        <h4 style="color: #517CB9; font-size: 30px"> 
		<center> 
			<a href="http://www.interspeech2020.org/"> <img src="./INTERSPEECH_2020_logo.png" style="max-width:80%"></a> 
		</center>
	</h4>
        </div>
      </div>

      <h3><center>
      <iframe width="100%" height="512" src="https://www.youtube.com/embed/ku9xoLh62E4" allowfullscreen></iframe>
      </center></h3>

      <div class="row">
      	<h3>Authors</h3>
      	<div style="font-size: 16px">
      	<ul>
		<li><a href="https://soowhanchung.github.io/">Soo-Whan Chung</a>, <i>Yonsei University</i></li>
		<li>Soyeon Choe, <i>Naver Corporation</i></li>
		<li>Joon Son Chung, <i>Naver Corporation</i></li>
		<li><a href="http://dsp.yonsei.ac.kr/">Hong-Goo Kang</a>, <i>Yonsei University</i></li>
      	</ul>
      	</div>

      	</div>

      	<div class="row"><h3>Abstract</h3>
		<p style="text-align: justify;">
		The objective of this paper is to separate a target speaker's speech from a mixture of two speakers using a deep audio-visual speech separation network. Unlike previous works that used lip movement on video clips or pre-enrolled speaker information as an auxiliary conditional feature, we use a single face image of the target speaker. In this task, the conditional feature is obtained from facial appearance in cross-modal biometric task, where audio and visual identity representations are shared in latent space. Learnt identities from facial images enforce the network to isolate matched speakers and extract the voices from mixed speech. It solves the permutation problem caused by swapped channel outputs, frequently occurred in speech separation tasks. The proposed method is far more practical than video-based speech separation since user profile images are readily available on many platforms. Also, unlike speaker-aware separation methods, it is applicable on separation with unseen speakers who have never been enrolled before. We show strong qualitative and quantitative results on challenging real-world examples.
		</p>  
      	</div>
      	<div class="row"><h3>Network Configuration</h3>
		<p style="text-align: justify;">
		<img src="./fig_block.png" style="max-width:100%;" align="center">
In this paper, we replace the speaker identity vector with cross-modal identity embeddings extracted from face images instead of that from pre-enrolled speakers’ voices. Without having a speaker preenrollment step, we can retrieve speaker identity from a profile image in the inference stage, even for unseen speaker’s voice. The network structure that we use is similar to the one proposed in <a href="https://www.robots.ox.ac.uk/~vgg/demo/theconversation/">[11]</a>, but with a few changes. Description of the proposed audio-visual speech separation network. Blue blocks are trainable neural networks whereas the red blocks are networks pre-trained in cross-modal biometric task. It consists of three sub-stages such as speech/image encoding stage, audio-visual fusion stage, and speech separation stage.
		<br><br>
        <br>
	</p>
	</div>
	<div class="row"><h3>Demo Samples</h3>
		<img src="./fig_spectrogram.PNG" style="max-width:100%;padding-right:20px;" align="center">
		Spectrogram for experimental results. There are 4 samples which are combinations of genders, where genders on the <b>left</b> are the targets and the <b>right</b> are set as interference. Each set demonstrates clean, mixed and separated speech from left to right respectively.
      	</div>

      <div class="row">
        <h3>Paper</h3>
	<p>
     </p><table>
  <tbody><tr></tr>
  <tr><td>
<!--     <a href="/"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="./thb.png" width="150px"></a> -->
	<i class="fa fa-pencil-square" aria-hidden="true"></i>
  </td>
  <td></td>
  <td>
    S.-W. Chung, S. Choe, J. S. Chung, H.-G. Kang<br>
    <b>FaceFilter: Audio-visual speech separation using still smages</b> <br>
    [<a href="https://doi.org/10.21437/Interspeech.2020-1065">Paper</a>] [<a href="/">Code</a>]
</td></tr></tbody></table>
     
      <h3>BibTeX</h3>
     <pre><tt>@inproceedings{chung2020facefilter,
  	title={FaceFilter: Audio-visual speech separation using still images},
  	author={Chung, Soo-Whan and Choe, Soyeon and Chung, Joon Son and Kang, Hong-Goo},
  	booktitle={INTERSPEECH},
  	year={2020}
	}</tt></pre>
      
    </div>
